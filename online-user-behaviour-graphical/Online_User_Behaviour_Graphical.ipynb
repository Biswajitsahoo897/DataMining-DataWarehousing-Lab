{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e93216db",
      "metadata": {},
      "source": [
        "# DMDW CASE STUDY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50ddcf4",
      "metadata": {},
      "source": [
        "# Online User Behaviour Analysis Graphical Model "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2265d899",
      "metadata": {},
      "source": [
        "### STEP 1. IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\biswa\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Libraries Imported Successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import sqlite3\n",
        "from hmmlearn import hmm\n",
        "import networkx as nx\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "from pgmpy.estimators import HillClimbSearch, BDeu\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\" Libraries Imported Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df12d4e7",
      "metadata": {},
      "source": [
        "### STEP 2 : LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows,Cols: (250000, 13)\n",
            "Dataset Loaded Successfully \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Purchase Date</th>\n",
              "      <th>Product Category</th>\n",
              "      <th>Product Price</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Total Purchase Amount</th>\n",
              "      <th>Payment Method</th>\n",
              "      <th>Customer Age</th>\n",
              "      <th>Returns</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>46251</td>\n",
              "      <td>2020-09-08 09:38:32</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>740</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Christine Hernandez</td>\n",
              "      <td>37</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>46251</td>\n",
              "      <td>2022-03-05 12:56:35</td>\n",
              "      <td>Home</td>\n",
              "      <td>468</td>\n",
              "      <td>4</td>\n",
              "      <td>2739</td>\n",
              "      <td>PayPal</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Christine Hernandez</td>\n",
              "      <td>37</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46251</td>\n",
              "      <td>2022-05-23 18:18:01</td>\n",
              "      <td>Home</td>\n",
              "      <td>288</td>\n",
              "      <td>2</td>\n",
              "      <td>3196</td>\n",
              "      <td>PayPal</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Christine Hernandez</td>\n",
              "      <td>37</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>46251</td>\n",
              "      <td>2020-11-12 13:13:29</td>\n",
              "      <td>Clothing</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>3509</td>\n",
              "      <td>PayPal</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Christine Hernandez</td>\n",
              "      <td>37</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13593</td>\n",
              "      <td>2020-11-27 17:55:11</td>\n",
              "      <td>Home</td>\n",
              "      <td>449</td>\n",
              "      <td>1</td>\n",
              "      <td>3452</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>James Grant</td>\n",
              "      <td>49</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer ID        Purchase Date Product Category  Product Price  Quantity  \\\n",
              "0        46251  2020-09-08 09:38:32      Electronics             12         3   \n",
              "1        46251  2022-03-05 12:56:35             Home            468         4   \n",
              "2        46251  2022-05-23 18:18:01             Home            288         2   \n",
              "3        46251  2020-11-12 13:13:29         Clothing            196         1   \n",
              "4        13593  2020-11-27 17:55:11             Home            449         1   \n",
              "\n",
              "   Total Purchase Amount Payment Method  Customer Age  Returns  \\\n",
              "0                    740    Credit Card            37      0.0   \n",
              "1                   2739         PayPal            37      0.0   \n",
              "2                   3196         PayPal            37      0.0   \n",
              "3                   3509         PayPal            37      0.0   \n",
              "4                   3452    Credit Card            49      0.0   \n",
              "\n",
              "         Customer Name  Age  Gender  Churn  \n",
              "0  Christine Hernandez   37    Male      0  \n",
              "1  Christine Hernandez   37    Male      0  \n",
              "2  Christine Hernandez   37    Male      0  \n",
              "3  Christine Hernandez   37    Male      0  \n",
              "4          James Grant   49  Female      1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('ecommerce_customer_data_custom_ratios.csv')\n",
        "print('Rows,Cols:', df.shape)\n",
        "print(\"Dataset Loaded Successfully \")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e0c063",
      "metadata": {},
      "source": [
        "###     STEP 3: DATA UNDERSTANDING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Understanding ---\n",
            "Shape: (250000, 13)\n",
            "\n",
            "Missing Values:\n",
            " Customer ID                  0\n",
            "Purchase Date                0\n",
            "Product Category             0\n",
            "Product Price                0\n",
            "Quantity                     0\n",
            "Total Purchase Amount        0\n",
            "Payment Method               0\n",
            "Customer Age                 0\n",
            "Returns                  47596\n",
            "Customer Name                0\n",
            "Age                          0\n",
            "Gender                       0\n",
            "Churn                        0\n",
            "dtype: int64\n",
            "\n",
            "Data Types:\n",
            " Customer ID                int64\n",
            "Purchase Date             object\n",
            "Product Category          object\n",
            "Product Price              int64\n",
            "Quantity                   int64\n",
            "Total Purchase Amount      int64\n",
            "Payment Method            object\n",
            "Customer Age               int64\n",
            "Returns                  float64\n",
            "Customer Name             object\n",
            "Age                        int64\n",
            "Gender                    object\n",
            "Churn                      int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Data Understanding ---\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "print(\"\\nData Types:\\n\", df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f794cc",
      "metadata": {},
      "source": [
        "### STEP 4: DATA CLEANING & PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Cleaning & Pre-processing ---\n",
            " Missing 'Returns' values filled with 0.\n",
            "Dropped columns for ML: ['Customer ID', 'Purchase Date', 'Customer Name', 'Age']\n",
            " Data Cleaned and Encoded for ML\n",
            "   Product Category  Product Price  Quantity  Total Purchase Amount  \\\n",
            "0                 2             12         3                    740   \n",
            "1                 3            468         4                   2739   \n",
            "2                 3            288         2                   3196   \n",
            "3                 1            196         1                   3509   \n",
            "4                 3            449         1                   3452   \n",
            "\n",
            "   Payment Method  Customer Age  Returns  Gender  Churn  \n",
            "0               1            37      0.0       1      0  \n",
            "1               3            37      0.0       1      0  \n",
            "2               3            37      0.0       1      0  \n",
            "3               3            37      0.0       1      0  \n",
            "4               1            49      0.0       0      1  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Data Cleaning & Pre-processing ---\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df['Returns'] = df['Returns'].fillna(0)\n",
        "print(\" Missing 'Returns' values filled with 0.\")\n",
        "\n",
        "df['Purchase Date'] = pd.to_datetime(df['Purchase Date'], errors='coerce')\n",
        "\n",
        "df_full = df.copy()\n",
        "\n",
        "cols_to_drop = ['Customer ID', 'Purchase Date', 'Customer Name', 'Age']\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "print(f\"Dropped columns for ML: {cols_to_drop}\")\n",
        "\n",
        "label_cols = df.select_dtypes(include=['object']).columns\n",
        "le_dict = {}\n",
        "for col in label_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    le_dict[col] = le\n",
        "\n",
        "print(\" Data Cleaned and Encoded for ML\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd38997",
      "metadata": {},
      "source": [
        "### STEP 5: ENHANCED ETL & DATA WAREHOUSE CREATION (SQLite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Creating Enhanced Data Warehouse with Time Dimension ---\n",
            " Created 'dim_date' (Time Dimension)\n",
            " Created 'dim_customer'\n",
            "Created 'dim_product'\n",
            " Created 'fact_behavior' with Time Dimension\n",
            " Enhanced Data Warehouse 'user_behavior.db' created successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Creating Enhanced Data Warehouse with Time Dimension ---\")\n",
        "conn = None \n",
        "try:\n",
        "    conn = sqlite3.connect('user_behavior.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create Time Dimension (CRITICAL for DW)\n",
        "    df_full['Year'] = df_full['Purchase Date'].dt.year\n",
        "    df_full['Month'] = df_full['Purchase Date'].dt.month\n",
        "    df_full['Quarter'] = df_full['Purchase Date'].dt.quarter\n",
        "    df_full['DayOfWeek'] = df_full['Purchase Date'].dt.day_name()\n",
        "    \n",
        "    dim_date = df_full[['Purchase Date', 'Year', 'Month', 'Quarter', 'DayOfWeek']].drop_duplicates()\n",
        "    dim_date['DateID'] = range(1, len(dim_date) + 1)\n",
        "    dim_date.to_sql('dim_date', conn, if_exists='replace', index=False)\n",
        "    print(\" Created 'dim_date' (Time Dimension)\")\n",
        "\n",
        "    # DimCustomer\n",
        "    dim_customer_cols = ['Customer ID', 'Customer Name', 'Customer Age', 'Gender']\n",
        "    dim_customer = df_full[dim_customer_cols].drop_duplicates(subset=['Customer ID']).reset_index(drop=True)\n",
        "    dim_customer.to_sql('dim_customer', conn, if_exists='replace', index=False)\n",
        "    print(\" Created 'dim_customer'\")\n",
        "\n",
        "    # DimProduct\n",
        "    dim_product = df_full[['Product Category']].drop_duplicates().reset_index(drop=True)\n",
        "    dim_product['ProductID'] = dim_product.index + 1 \n",
        "    dim_product = dim_product[['ProductID', 'Product Category']]\n",
        "    dim_product.to_sql('dim_product', conn, if_exists='replace', index=False)\n",
        "    print(\"Created 'dim_product'\")\n",
        "    \n",
        "    # Create Enhanced Fact Table with DateID\n",
        "    fact_behavior = pd.merge(df_full, dim_product, on='Product Category')\n",
        "    fact_behavior = pd.merge(fact_behavior, dim_date[['Purchase Date', 'DateID']], on='Purchase Date')\n",
        "    \n",
        "    fact_columns = [\n",
        "        'Customer ID', 'ProductID', 'DateID', \n",
        "        'Product Price', 'Quantity', 'Total Purchase Amount', 'Payment Method',\n",
        "        'Returns', 'Churn'\n",
        "    ]\n",
        "    fact_behavior = fact_behavior[fact_columns]\n",
        "    fact_behavior.to_sql('fact_behavior', conn, if_exists='replace', index=False)\n",
        "    print(\" Created 'fact_behavior' with Time Dimension\")\n",
        "\n",
        "    conn.commit()\n",
        "    print(\" Enhanced Data Warehouse 'user_behavior.db' created successfully.\")\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\" Error creating DW: A column {e} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\" Error creating DW: {e}\")\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708999a7",
      "metadata": {},
      "source": [
        "### STEP 6: COMPREHENSIVE OLAP QUERIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Multiple OLAP Operations ---\n",
            "\n",
            " OLAP Query 1: Revenue by Category and Gender\n",
            "  Product Category  Gender  TotalSpend  TotalOrders  AvgOrderValue\n",
            "0         Clothing  Female   103723243        37946    2733.443393\n",
            "1            Books  Female   102518477        37473    2735.795826\n",
            "2            Books    Male   102421124        37439    2735.680013\n",
            "3         Clothing    Male   100809162        37106    2716.788713\n",
            "4      Electronics    Male    68372213        25057    2728.667159\n",
            "5      Electronics  Female    68227254        25128    2715.188395\n",
            "6             Home  Female    67993447        25013    2718.324351\n",
            "7             Home    Male    67277763        24838    2708.662654\n",
            "\n",
            " OLAP Query 2: Quarterly Revenue Trend (DRILL-DOWN)\n",
            "    Year  Quarter  QuarterlyRevenue  TotalTransactions\n",
            "0   2020        1          45541363              16764\n",
            "1   2020        2          45224012              16772\n",
            "2   2020        3          46928563              17211\n",
            "3   2020        4          47582236              17298\n",
            "4   2021        1          45767972              16754\n",
            "5   2021        2          45577688              16719\n",
            "6   2021        3          45658935              16731\n",
            "7   2021        4          45838808              16895\n",
            "8   2022        1          45858057              16714\n",
            "9   2022        2          45342678              16710\n",
            "10  2022        3          46002147              16999\n",
            "11  2022        4          45722620              16746\n",
            "12  2023        1          45744435              16709\n",
            "13  2023        2          46000164              16849\n",
            "14  2023        3          38553005              14129\n",
            "\n",
            " OLAP Query 3: Churn Analysis by Product Category (SLICE)\n",
            "  Product Category  ChurnedCustomers  TotalCustomers  ChurnRate\n",
            "0             Home             10071           49851      20.20\n",
            "1            Books             14926           74912      19.92\n",
            "2      Electronics              9979           50185      19.88\n",
            "3         Clothing             14898           75052      19.85\n",
            "\n",
            " OLAP Query 4: Payment Method Distribution by Quarter (PIVOT)\n",
            "  Payment Method  Q1_Revenue  Q2_Revenue  Q3_Revenue  Q4_Revenue\n",
            "0           Cash    36539267    36138899    35259610    27767775\n",
            "1    Credit Card    73955967    72872295    70976882    56347252\n",
            "2         Crypto    18033569    18329179    17637766    13457609\n",
            "3         PayPal    54383024    54804169    53268392    41571028\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Running Multiple OLAP Operations ---\")\n",
        "conn = None\n",
        "try:\n",
        "    conn = sqlite3.connect('user_behavior.db')\n",
        "    \n",
        "    # OLAP 1: Basic Aggregation with Grouping\n",
        "    print(\"\\n OLAP Query 1: Revenue by Category and Gender\")\n",
        "    olap_query1 = \"\"\"\n",
        "    SELECT\n",
        "        p.\"Product Category\",\n",
        "        c.Gender,\n",
        "        SUM(f.\"Total Purchase Amount\") AS TotalSpend,\n",
        "        COUNT(*) AS TotalOrders,\n",
        "        AVG(f.\"Total Purchase Amount\") AS AvgOrderValue\n",
        "    FROM fact_behavior f\n",
        "    JOIN dim_customer c ON f.\"Customer ID\" = c.\"Customer ID\"\n",
        "    JOIN dim_product p ON f.ProductID = p.ProductID\n",
        "    GROUP BY p.\"Product Category\", c.Gender\n",
        "    ORDER BY TotalSpend DESC\n",
        "    LIMIT 10;\n",
        "    \"\"\"\n",
        "    olap_result1 = pd.read_sql_query(olap_query1, conn)\n",
        "    print(olap_result1)\n",
        "\n",
        "    # OLAP 2: Time-based Analysis (Drill-Down)\n",
        "    print(\"\\n OLAP Query 2: Quarterly Revenue Trend (DRILL-DOWN)\")\n",
        "    olap_query2 = \"\"\"\n",
        "    SELECT\n",
        "        d.Year,\n",
        "        d.Quarter,\n",
        "        SUM(f.\"Total Purchase Amount\") AS QuarterlyRevenue,\n",
        "        COUNT(*) AS TotalTransactions\n",
        "    FROM fact_behavior f\n",
        "    JOIN dim_date d ON f.DateID = d.DateID\n",
        "    GROUP BY d.Year, d.Quarter\n",
        "    ORDER BY d.Year, d.Quarter;\n",
        "    \"\"\"\n",
        "    olap_result2 = pd.read_sql_query(olap_query2, conn)\n",
        "    print(olap_result2)\n",
        "\n",
        "    # OLAP 3: Slice Operation\n",
        "    print(\"\\n OLAP Query 3: Churn Analysis by Product Category (SLICE)\")\n",
        "    olap_query3 = \"\"\"\n",
        "    SELECT\n",
        "        p.\"Product Category\",\n",
        "        SUM(CASE WHEN f.Churn = 1 THEN 1 ELSE 0 END) AS ChurnedCustomers,\n",
        "        COUNT(*) AS TotalCustomers,\n",
        "        ROUND(100.0 * SUM(CASE WHEN f.Churn = 1 THEN 1 ELSE 0 END) / COUNT(*), 2) AS ChurnRate\n",
        "    FROM fact_behavior f\n",
        "    JOIN dim_product p ON f.ProductID = p.ProductID\n",
        "    GROUP BY p.\"Product Category\"\n",
        "    ORDER BY ChurnRate DESC;\n",
        "    \"\"\"\n",
        "    olap_result3 = pd.read_sql_query(olap_query3, conn)\n",
        "    print(olap_result3)\n",
        "\n",
        "    # OLAP 4: Pivot-style Query\n",
        "    print(\"\\n OLAP Query 4: Payment Method Distribution by Quarter (PIVOT)\")\n",
        "    olap_query4 = \"\"\"\n",
        "    SELECT\n",
        "        f.\"Payment Method\",\n",
        "        SUM(CASE WHEN d.Quarter = 1 THEN f.\"Total Purchase Amount\" ELSE 0 END) AS Q1_Revenue,\n",
        "        SUM(CASE WHEN d.Quarter = 2 THEN f.\"Total Purchase Amount\" ELSE 0 END) AS Q2_Revenue,\n",
        "        SUM(CASE WHEN d.Quarter = 3 THEN f.\"Total Purchase Amount\" ELSE 0 END) AS Q3_Revenue,\n",
        "        SUM(CASE WHEN d.Quarter = 4 THEN f.\"Total Purchase Amount\" ELSE 0 END) AS Q4_Revenue\n",
        "    FROM fact_behavior f\n",
        "    JOIN dim_date d ON f.DateID = d.DateID\n",
        "    GROUP BY f.\"Payment Method\";\n",
        "    \"\"\"\n",
        "    olap_result4 = pd.read_sql_query(olap_query4, conn)\n",
        "    print(olap_result4)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error running OLAP queries: {e}\")\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3259374",
      "metadata": {},
      "source": [
        "### STEP 7: HMM SESSIONIZATION & MODELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting HMM Sessionization ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Prepared 49673 sequences for HMM.\n",
            "--- Training HMM ---\n",
            " HMM Model Trained.\n",
            " Saved 'plot_hmm_transition_matrix.png'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"\\n--- Starting HMM Sessionization ---\")\n",
        "try:\n",
        "    le_hmm = LabelEncoder()\n",
        "    df_full['Category_Code'] = le_hmm.fit_transform(df_full['Product Category'])\n",
        "\n",
        "    df_full.sort_values(by=['Customer ID', 'Purchase Date'], inplace=True)\n",
        "\n",
        "    sequences = df_full.groupby('Customer ID')['Category_Code'].apply(list)\n",
        "    sequences = sequences[sequences.apply(len) > 0]\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    \n",
        "    X_hmm = np.concatenate(sequences.values).reshape(-1, 1)\n",
        "\n",
        "    print(f\" Prepared {len(lengths)} sequences for HMM.\")\n",
        "\n",
        "    n_hidden_states = 4\n",
        "    hmm_model = hmm.MultinomialHMM(n_components=n_hidden_states, random_state=42, n_iter=100)\n",
        "    print(\"--- Training HMM ---\")\n",
        "    hmm_model.fit(X_hmm, lengths)\n",
        "    print(\" HMM Model Trained.\")\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.heatmap(hmm_model.transmat_, annot=True, cmap='viridis', fmt='.3f')\n",
        "    plt.title(f'HMM Hidden State Transition Matrix (k={n_hidden_states})')\n",
        "    plt.xlabel('To State')\n",
        "    plt.ylabel('From State')\n",
        "    plt.savefig(\"plot_hmm_transition_matrix.png\")\n",
        "    print(\" Saved 'plot_hmm_transition_matrix.png'\")\n",
        "    plt.clf()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error during HMM Step: {e}\")\n",
        "    print(\"--- Skipping HMM and continuing script ---\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63c3eae",
      "metadata": {},
      "source": [
        "### STEP 8: BAYESIAN NETWORK (Structure Learning) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f04ab66",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
            " {'Total Purchase Amount': 'O', 'Quantity': 'O', 'Returns': 'O', 'Payment Method': 'C', 'Customer Age': 'O', 'Churn': 'C'}\n",
            "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
            " {'Total Purchase Amount': 'O', 'Quantity': 'O', 'Returns': 'O', 'Payment Method': 'C', 'Customer Age': 'O', 'Churn': 'C'}\n",
            "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
            " {'Total Purchase Amount': 'O', 'Quantity': 'O', 'Returns': 'O', 'Payment Method': 'C', 'Customer Age': 'O', 'Churn': 'C'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Learning Bayesian Network Structure ---\n",
            "--- Discretizing continuous variables ---\n",
            " Discretized data for Bayesian Network\n",
            "--- Starting Hill Climb Search ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1000 [00:00<06:51,  2.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Bayesian Network Created. Edges: [('Customer Age', 'Total Purchase Amount')]\n",
            " Saved 'plot_bayesian_network_graph.png'\n",
            "\n",
            " Bayesian Network Edges:\n",
            "   Customer Age → Total Purchase Amount\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Learning Bayesian Network Structure ---\")\n",
        "try:\n",
        "    bn_cols = ['Total Purchase Amount', 'Quantity', 'Returns', 'Payment Method', 'Customer Age', 'Churn']\n",
        "    df_bn = df[bn_cols].copy() \n",
        "\n",
        "    # Discretize continuous variables for pgmpy\n",
        "    print(\"--- Discretizing continuous variables ---\")\n",
        "    df_bn['Total Purchase Amount'] = pd.cut(df_bn['Total Purchase Amount'], \n",
        "                                             bins=5, labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'])\n",
        "    df_bn['Quantity'] = pd.cut(df_bn['Quantity'], bins=3, labels=['Low', 'Medium', 'High'])\n",
        "    df_bn['Customer Age'] = pd.cut(df_bn['Customer Age'], bins=4, labels=['Young', 'Adult', 'MiddleAged', 'Senior'])\n",
        "    df_bn['Returns'] = pd.cut(df_bn['Returns'], bins=3, labels=['NoReturns', 'FewReturns', 'ManyReturns'])\n",
        "\n",
        "    for col in df_bn.columns:\n",
        "        df_bn[col] = df_bn[col].astype('category')\n",
        "\n",
        "    print(f\" Discretized data for Bayesian Network\")\n",
        "\n",
        "    print(\"--- Starting Hill Climb Search ---\")\n",
        "    hc = HillClimbSearch(df_bn)\n",
        "    scoring_method = BDeu(data=df_bn, equivalent_sample_size=50)\n",
        "    best_model_structure = hc.estimate(scoring_method=scoring_method, max_iter=1000)\n",
        "    \n",
        "    if len(best_model_structure.edges()) == 0:\n",
        "        print(\" No edges found. Creating manual structure.\")\n",
        "        bn_model = DiscreteBayesianNetwork([\n",
        "            ('Payment Method', 'Churn'),\n",
        "            ('Total Purchase Amount', 'Churn'),\n",
        "            ('Customer Age', 'Churn'),\n",
        "            ('Quantity', 'Returns'),\n",
        "            ('Returns', 'Churn')\n",
        "        ])\n",
        "    else:\n",
        "        bn_model = DiscreteBayesianNetwork(best_model_structure.edges())\n",
        "\n",
        "    print(f\" Bayesian Network Created. Edges: {list(bn_model.edges())}\")\n",
        "\n",
        "    if len(bn_model.edges()) > 0:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        G = nx.DiGraph()\n",
        "        G.add_edges_from(bn_model.edges())\n",
        "        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "        \n",
        "        nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='lightblue', \n",
        "                               alpha=0.9, linewidths=2, edgecolors='navy')\n",
        "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
        "        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, \n",
        "                               arrowsize=20, arrowstyle='->', width=2,\n",
        "                               connectionstyle='arc3,rad=0.1')\n",
        "        \n",
        "        plt.title(\"Learned Bayesian Network Structure\", fontsize=16, fontweight='bold')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plot_bayesian_network_graph.png\", dpi=300, bbox_inches='tight')\n",
        "        print(\" Saved 'plot_bayesian_network_graph.png'\")\n",
        "        plt.clf()\n",
        "        \n",
        "        print(\"\\n Bayesian Network Edges:\")\n",
        "        for edge in bn_model.edges():\n",
        "            print(f\"   {edge[0]} → {edge[1]}\")\n",
        "    else:\n",
        "        print(\" No edges to visualize\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error during Bayesian Network: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091fa2f3",
      "metadata": {},
      "source": [
        "### STEP 9: EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Visualization ---\n",
            " Saved 'plot_histograms.png'\n",
            " Saved 'plot_heatmap.png'\n",
            " Saved 'plot_churn_distribution.png'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"\\n--- Starting Visualization ---\")\n",
        "\n",
        "df.hist(figsize=(12, 8), bins=20, color='skyblue', edgecolor='black')\n",
        "plt.suptitle(\"Distribution of Numeric Features\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.savefig(\"plot_histograms.png\")\n",
        "print(\" Saved 'plot_histograms.png'\")\n",
        "plt.clf()\n",
        "\n",
        "plt.figure(figsize=(12, 8)) \n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".1f\", annot_kws={\"size\": 8})\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"plot_heatmap.png\")\n",
        "print(\" Saved 'plot_heatmap.png'\")\n",
        "plt.clf()\n",
        "\n",
        "if 'Churn' in df.columns:\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    df['Churn'].value_counts().plot(kind='bar', color=['lightgreen', 'salmon'])\n",
        "    plt.title(\"Target Class Distribution (Churn)\")\n",
        "    plt.xlabel(\"0 = Stay | 1 = Churn\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.savefig(\"plot_churn_distribution.png\")\n",
        "    print(\" Saved 'plot_churn_distribution.png'\")\n",
        "    plt.clf()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97a54aa",
      "metadata": {},
      "source": [
        "### STEP 10: FEATURE SELECTION & SCALING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Features Selected and Scaled\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns) \n",
        "print(\"\\n Features Selected and Scaled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e76dc72",
      "metadata": {},
      "source": [
        "### STEP 11: K-MEANS CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Finding Optimal K for K-Means ---\n",
            " Saved 'plot_kmeans_elbow.png'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"\\n--- Finding Optimal K for K-Means ---\")\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K, inertia, 'bx-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.savefig(\"plot_kmeans_elbow.png\")\n",
        "print(\" Saved 'plot_kmeans_elbow.png'\")\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2236ebe7",
      "metadata": {},
      "source": [
        "### STEP 12: K-MEANS SEGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " K-Means Clustering Complete. 4 clusters created.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved 'plot_cluster_churn_distribution.png'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "k_optimal = 4\n",
        "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
        "kmeans.fit(X_scaled)\n",
        "df['Cluster'] = kmeans.labels_\n",
        "print(f\"\\n K-Means Clustering Complete. {k_optimal} clusters created.\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Cluster', hue='Churn', data=df, palette=['lightgreen', 'salmon'])\n",
        "plt.title('Churn Distribution Within Each Customer Cluster')\n",
        "plt.xlabel('Customer Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Churn', labels=['0 = Stay', '1 = Churn'])\n",
        "plt.savefig(\"plot_cluster_churn_distribution.png\")\n",
        "print(\" Saved 'plot_cluster_churn_distribution.png'\")\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15bd28c2",
      "metadata": {},
      "source": [
        "### STEP 13: TRAIN-TEST SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ccb1e424",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Data: (200000, 8)\n",
            "Testing Data: (50000, 8)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nTraining Data:\", X_train.shape)\n",
        "print(\"Testing Data:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2cbfad",
      "metadata": {},
      "source": [
        "### STEP 14: MODEL TRAINING (Random Forest + Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f5c91533",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Random Forest ---\n",
            " Random Forest Training Complete\n",
            "--- Training Decision Tree ---\n",
            " Decision Tree Training Complete\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "print(\"\\n--- Training Random Forest ---\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\" Random Forest Training Complete\")\n",
        "\n",
        "# Decision Tree\n",
        "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "print(\"--- Training Decision Tree ---\")\n",
        "dt_model.fit(X_train, y_train)\n",
        "print(\" Decision Tree Training Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2eb1b93",
      "metadata": {},
      "source": [
        "### STEP 15: MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "71c0069a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Random Forest Evaluation:\n",
            "Accuracy: 79.35 %\n",
            "F1 Score: 0.712\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.99      0.88     40016\n",
            "           1       0.20      0.01      0.02      9984\n",
            "\n",
            "    accuracy                           0.79     50000\n",
            "   macro avg       0.50      0.50      0.45     50000\n",
            "weighted avg       0.68      0.79      0.71     50000\n",
            "\n",
            " Saved 'plot_confusion_matrix_rf.png'\n",
            "\n",
            " Decision Tree Accuracy: 80.03 %\n",
            " Saved 'plot_confusion_matrix_dt.png'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Random Forest Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\n Random Forest Evaluation:\")\n",
        "print(\"Accuracy:\", round(acc * 100, 2), \"%\")\n",
        "print(\"F1 Score:\", round(f1, 3))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"plot_confusion_matrix_rf.png\")\n",
        "print(\" Saved 'plot_confusion_matrix_rf.png'\")\n",
        "plt.clf()\n",
        "\n",
        "# Decision Tree Evaluation\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"\\n Decision Tree Accuracy:\", round(acc_dt * 100, 2), \"%\")\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Decision Tree\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"plot_confusion_matrix_dt.png\")\n",
        "print(\" Saved 'plot_confusion_matrix_dt.png'\")\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6cebb1",
      "metadata": {},
      "source": [
        "### STEP 16: FEATURE IMPORTANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edb0a91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 'plot_feature_importance.png'\n",
            " Saved 'plot_decision_tree.png'\n",
            "\n",
            " --- Script Finished Successfully ---\n",
            "\n",
            " Summary of Outputs:\n",
            "  - Data Warehouse: user_behavior.db (with Time Dimension)\n",
            "  - OLAP Queries: 4 comprehensive queries executed\n",
            "  - HMM Model: Trained and transition matrix saved\n",
            "  - Bayesian Network: Structure learned and visualized\n",
            "  - Clustering: K-Means with 4 clusters\n",
            "  - Classification: Random Forest + Decision Tree\n",
            "  - Visualizations: 11 plots saved\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "feature_names = X.columns \n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=importances[indices], y=feature_names[indices], palette='viridis')\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.savefig(\"plot_feature_importance.png\")\n",
        "print(\"Saved 'plot_feature_importance.png'\")\n",
        "plt.clf()\n",
        "\n",
        "# Decision Tree Visualization\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_model, feature_names=X.columns, class_names=['Stay', 'Churn'], \n",
        "            filled=True, rounded=True, fontsize=10)\n",
        "plt.title('Decision Tree for Churn Prediction')\n",
        "plt.savefig('plot_decision_tree.png', dpi=300, bbox_inches='tight')\n",
        "print(\" Saved 'plot_decision_tree.png'\")\n",
        "plt.clf()\n",
        "\n",
        "print(\"\\n --- Script Finished Successfully ---\")\n",
        "print(\"\\n Summary of Outputs:\")\n",
        "print(\"  - Data Warehouse: user_behavior.db (with Time Dimension)\")\n",
        "print(\"  - OLAP Queries: 4 comprehensive queries executed\")\n",
        "print(\"  - HMM Model: Trained and transition matrix saved\")\n",
        "print(\"  - Bayesian Network: Structure learned and visualized\")\n",
        "print(\"  - Clustering: K-Means with 4 clusters\")\n",
        "print(\"  - Classification: Random Forest + Decision Tree\")\n",
        "print(\"  - Visualizations: 11 plots saved\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
